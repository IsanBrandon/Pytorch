{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9d93ac",
   "metadata": {},
   "source": [
    "# 1. Tensor의 형태 변환\n",
    "## 1.1. Tensor의 형태 변경 - reshape\n",
    "reshape()함수는 Tensor의 형태를 변경하는 데 사용됩니다. 이 함수는 원본 Tensor의 data를 유지하면서 새로운 형태의 Tensor를 반환합니다. \\\n",
    "\\\n",
    "reshape()는 주어진 새로운 형태가 원본 Tensor의 총 원소 수와 일치할 때 사용할 수 있습니다. 이는 data의 차원을 재구성하거나, 특정 연산에 적합한 형태로 Tensor를 조정할 필요가 있을 때 매우 유용합니다. \\\n",
    "\\\n",
    "만약 변경하려는 형태가 원본 Tensor의 data와 호환되면, PyTorch는 가능한 한 원본 data를 공유하려고 시도합니다. 만약 원본 Tensor의 memory layout이 연속적이지 않아서 직접적인 형태 변경이 불가능한 경우, reshape()는 먼저 Tensor의 data를 복사하여 memory상에서 연속적인 형태로 만든 다음 형태를 변경합니다. 이는 추가적인 memory 사용과 계산 비용을 발생시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80bc16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서:  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "텐서의 형태:  torch.Size([16])\n",
      "\n",
      "변형 후 텐서:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "변형 후 텐서의 형태:  torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 초기 텐서 생성\n",
    "x = torch.arange(16)    # 0에서 15까지의 숫자를 포함하는 텐서\n",
    "print(\"텐서: \", x)\n",
    "print(\"텐서의 형태: \", x.shape)\n",
    "\n",
    "# reshape 함수 사용 예제\n",
    "reshaped_x = x.reshape(4, 4)    # 4x4 텐서로 변경\n",
    "print(\"\\n변형 후 텐서:\\n\", reshaped_x)\n",
    "print(\"변형 후 텐서의 형태: \", reshaped_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c4601",
   "metadata": {},
   "source": [
    "## 1.2. Tensor의 형태 변경 - view\n",
    "view()함수는 원본 Tensor의 data를 공유하는 새로운 형태의 Tensor를 반환합니다. 이는 data의 실제 memory layout을 변경하지 않으며, 단지 Tensor에 대한 새로운 index view를 제공합니다. \\\n",
    "\\\n",
    "만약 원본 Tensor가 memory상에서 연속적이지 않은 경우, view()함수는 오류를 발생시킵니다. 이런 상황에서는 reshape()함수를 사용하거나, contiguous() 메소드를 호출한 후 view()를 사용해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca93adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변형 후 텐서:\n",
      " tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15]]])\n",
      "변형 후 텐서의 형태:  torch.Size([4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "viewed_x = x.view(4, 2, 2)  # 4x2x2 텐서롤 변경\n",
    "print(\"변형 후 텐서:\\n\", viewed_x)\n",
    "print(\"변형 후 텐서의 형태: \", viewed_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960adbe",
   "metadata": {},
   "source": [
    "# 1.3. 차원 축소 - squeeze\n",
    "squeeze함수는 Tensor에서 크기가 1인 차원을 찾아서 그 차원을 제거합니다. 이는 data의 형태를 바꾸지 않으면서 Tensor를 더 효율적으로 만들어 줍니다. \\\n",
    "\\\n",
    "예를 들어, 만약 여러분이 [1, 10, 1, 50] 크기의 Tensor를 가지고 있다면, sqeeze를 사용한 후에는 [10, 50] 크기의 Tensor를 얻게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c0dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텐서:  tensor([[[1, 2, 3, 4]]])\n",
      "원본 텐서 형태:  torch.Size([1, 1, 4])\n",
      "\n",
      "변형된 텐서:  tensor([1, 2, 3, 4])\n",
      "변형된 텐서 형태:  torch.Size([4])\n",
      "\n",
      "변형된 텐서:  tensor([[1, 2, 3, 4]])\n",
      "변형된 텐서 형태:  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 1인 차원을 포함하는 텐서 생성\n",
    "tensor = torch.tensor([[[1, 2, 3, 4]]])     # 크기: (1, 1, 4)\n",
    "print(\"원본 텐서: \", tensor)\n",
    "print(\"원본 텐서 형태: \", tensor.shape)\n",
    "\n",
    "# 모든 크기가 1인 차원을 제거\n",
    "squeezed_tensor = tensor.squeeze()\n",
    "print(\"\\n변형된 텐서: \", squeezed_tensor)\n",
    "print(\"변형된 텐서 형태: \", squeezed_tensor.shape)\n",
    "\n",
    "# 특정 차원을 지정하여 squeeze\n",
    "squeezed_dim_tensor = tensor.squeeze(0)     # 첫 번째 차원을 지정\n",
    "print(\"\\n변형된 텐서: \", squeezed_dim_tensor)\n",
    "print(\"변형된 텐서 형태: \", squeezed_dim_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f7e47",
   "metadata": {},
   "source": [
    "## 1.4. 차원 확장 - unsqueeze\n",
    "squeeze가 상자에서 불필요한 공간을 제거한다면, unsqueeze는 상자에 조금 더 공간을 추가하는 것과 비슷합니다. PyTorch에서 unsqueeze함수는 지정된 위치에 새로운 차원을 추가합니다. 이것은 data의 형태를 바꾸는 방법으로 특정 연산을 수행하기 위해 data의 차원을 조정해야 할 때 유용합니다. \\\n",
    "\\\n",
    "예를 들어, [10, 50] 크기의 Tensor에 unsqueeze를 사용하여 첫 번째 위치에 차원을 추가하면, [1, 10, 50]의 크기를 가진 Tensor가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b644ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텐서:  tensor([1, 2, 3, 4])\n",
      "원본 텐서 형태:  torch.Size([4])\n",
      "\n",
      "변형된 텐서:  tensor([[1, 2, 3, 4]])\n",
      "변형된 텐서 형태:  torch.Size([1, 4])\n",
      "\n",
      "변형된 텐서:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "변형된 텐서 형태:\n",
      " tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "# 원본 텐서 생성\n",
    "tensor = torch.tensor([1, 2, 3, 4])     # 크기: (4,)\n",
    "print(\"원본 텐서: \", tensor)\n",
    "print(\"원본 텐서 형태: \", tensor.shape)\n",
    "\n",
    "# 첫 번째 차원을 추가하여 차원 확장\n",
    "unsqueeze_tensor_first_dim = tensor.unsqueeze(0)\n",
    "print(\"\\n변형된 텐서: \", unsqueeze_tensor_first_dim)\n",
    "print(\"변형된 텐서 형태: \", unsqueeze_tensor_first_dim.shape)\n",
    "\n",
    "# 마지막 차원을 추가하여 차원 확장\n",
    "unsqueeze_tensor_last_dim = tensor.unsqueeze(-1)\n",
    "print(\"\\n변형된 텐서:\\n\", unsqueeze_tensor_last_dim)\n",
    "print(\"변형된 텐서 형태:\\n\", unsqueeze_tensor_last_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22212140",
   "metadata": {},
   "source": [
    "## 1.5. Tensor의 차원 교환 - transpose\n",
    "transpose 함수는 여러분이 책장을 정리하다가 책들을 가로로 놓은 것을 세로로 바꾸고 싶을 때와 비슷한 일을 합니다. \\\n",
    "즉, Tensor의 두 차원을 서로 바꾸는 역할을 합니다. 이는 matrix의 행과 열을 바꾸는 것과 유사하며, data를 다른 방식으로 해석하고 싶을 때 매우 유용합니다. \\\n",
    "\\\n",
    "예를 들어, [10, 50] 크기의 Tensor에 대해 첫 번째 차원과 두 번째 차원을 transpose하면, [50, 10] 크기의 Tensor를 얻게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342c8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변형된 텐서 형태:  torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# 초기 3차원 텐서 생성\n",
    "tensor = torch.rand(2, 3, 4)\n",
    "\n",
    "transposed_tensor = tensor.transpose(0, 1)  # 첫 번째 차원과 두 번째 차원 교환\n",
    "print(\"변형된 텐서 형태: \", transposed_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a153e6f",
   "metadata": {},
   "source": [
    "# 1.6. Tensor의 차원 교환 - permute\n",
    "permute 함수는 여러분이 방의 가구를 재배치하여 완전히 새로운 공간을 만들고 싶을 때와 비슷한 작업을 합니다.\\\n",
    "이 함수는 Tensor의 차원을 재배열할 수 있게 해줍니다. permute는 여러 차원을 한 번에 자유롭게 바꾸고 싶을 때 사용합니다.\\\n",
    "\\\n",
    "예를 들어, [10, 20, 30] 크기의 Tensor가 있을 때, permute를 사용하여 차원의 순서를 [2, 0, 1]로 바꾸면, [30, 10, 20] 크기의 새로운 형태의 Tensor를 얻게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6f6a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변형된 텐서 형태:  tensor([[[0.6902, 0.9641, 0.4081],\n",
      "         [0.0251, 0.9907, 0.7430]],\n",
      "\n",
      "        [[0.3498, 0.6869, 0.7054],\n",
      "         [0.0582, 0.9558, 0.0034]],\n",
      "\n",
      "        [[0.0644, 0.4187, 0.4528],\n",
      "         [0.3291, 0.1312, 0.4021]],\n",
      "\n",
      "        [[0.7887, 0.2753, 0.2252],\n",
      "         [0.6035, 0.0359, 0.8895]]])\n"
     ]
    }
   ],
   "source": [
    "# permute 사용 예제\n",
    "permuted_tensor = tensor.permute(2, 0, 1)   # 차원 순서를 (2, 0, 1)로 재배열\n",
    "print(\"변형된 텐서 형태: \", permuted_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff17833",
   "metadata": {},
   "source": [
    "# 2. Tensor의 결합과 분할\n",
    "## 2.1. Tenosr 결합 - cat\n",
    "cat 함수는 'concatenate(연결)'의 줄임말입니다. 마치 여러분이 친구들과 손을 잡고 줄을 서는 것처럼, cat 함수는 여러 Tensor를 지정된 차원을 따라 하나로 연결합니다.\\\n",
    "이를 통해 크기가 더 큰 새로운 Tensor를 생성할 수 있습니다. cat은 여러 data 조각을 하나로 모으고 싶을 때 매우 유용합니다.\\\n",
    "\\\n",
    "예를 들어, [2, 2] 및 [2, 3] 크기의 두 Tensor를 1번째 차원(열)을 따라 연결하면, [2, 5] 크기의 Tenosr가 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "159e4032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결합된 텐서:\n",
      " tensor([[ 1,  2,  5,  6,  7],\n",
      "        [ 3,  4,  8,  9, 10]])\n",
      "\n",
      "tensor1 텐서 형태:  torch.Size([2, 2])\n",
      "tensor2 텐서 형태:  torch.Size([2, 3])\n",
      "cat_tensors 텐서 형태:  torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# 두 개의 텐서 생성\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6, 7], [8, 9, 10]])\n",
    "\n",
    "# cat 함수 사용 예제\n",
    "cat_tensors = torch.cat((tensor1, tensor2), dim=1)  # 1번 차원(열)을 따라 결합\n",
    "print(\"결합된 텐서:\\n\", cat_tensors)\n",
    "\n",
    "print('\\ntensor1 텐서 형태: ', tensor1.shape)\n",
    "print('tensor2 텐서 형태: ', tensor2.shape)\n",
    "print('cat_tensors 텐서 형태: ', cat_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed2dd8",
   "metadata": {},
   "source": [
    "## 2.2. Tensor 결합 - stack\n",
    "stack 함수는 여러분이 책을 쌓아 올리듯이, 여러 Tensor를 새로운 차원을 추가하여 쌓아 올립니다.\\\n",
    "이는 cat과 다르게, 모든 Tensor에 대해 새로운 차원이 생기므로, Tenosor들은 그 크기가 동일해야만 합니다.\\\n",
    "stack은 data를 구조적으로 정리하고 싶을 때 유용합니다.\\\n",
    "\\\n",
    "예를 들어, 세 개의 [2, 2] 크기의 Tenosor를 stack으로 쌓으면, [3, 2, 2] 크기의 Tensor가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1972588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack_tensors\n",
      " tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]])\n",
      "stack_tensors 텐서 형태:  torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 2x2 Matrix 생성\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "tensor3 = torch.tensor([[9, 10], [11, 12]])\n",
    "\n",
    "# stack 함수 사용 예제\n",
    "stack_tensors = torch.stack((tensor1, tensor2, tensor3), dim=0) # 새 차원을 0번 차원으로 삽입하여 쌓기\n",
    "\n",
    "print(\"stack_tensors\\n\", stack_tensors)\n",
    "print(\"stack_tensors 텐서 형태: \", stack_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b44c8c",
   "metadata": {},
   "source": [
    "## 2.3. Tenosr 분할 - chunk\n",
    "chunk 함수는 큰 Tensor를 여러 개의 작은 Tensor로 나누는 데 사용됩니다. 마치 큰 케이크를 여러 조각으로 나누어 친구들과 나누어 먹는 것과 비슷합니다. \\\n",
    "이 함수는 두 개의 인자를 받습니다. 하나는 몇 개의 조각으로 나눌 것인지, 다른 하나는 어느 차원을 따라 나눌 것인지를 지정합니다. \\\n",
    "\\\n",
    "예를 들어, [10] 크기의 Tensor를 3개의 조각으로 나누면, 각각 [4] 크기의 Tensor 2개와 [2] 크기의 Tenosr 1개를 얻게 됩니다. \\\n",
    "chunk는 data를 작은 단위로 나누어 처리하고 싶을 때 매우 편리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33f6671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텐서:  tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "텐서 형태:  torch.Size([10])\n",
      "\n",
      " Chunks\n",
      "Chunk 0: tensor([0, 1, 2, 3]), torch.Size([4])\n",
      "Chunk 1: tensor([4, 5, 6, 7]), torch.Size([4])\n",
      "Chunk 2: tensor([8, 9]), torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 10개의 요소를 가진 1차원 텐서 생성\n",
    "tensor = torch.arange(10)   # 0부터 9까지의 숫자\n",
    "print(\"원본 텐서: \", tensor)\n",
    "print(\"텐서 형태: \", tensor.shape)\n",
    "\n",
    "# chuck 함수 사용 예제\n",
    "chucks = torch.chunk(tensor, 3, dim=0)  # 3개의 조각으로 분할\n",
    "print(\"\\n Chunks\")\n",
    "for i, chunk in enumerate(chucks):\n",
    "    print(f\"Chunk {i}: {chunk}, {chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811cc0e",
   "metadata": {},
   "source": [
    "## 2.4. Tensor 분할 - split\n",
    "split 함수는 chunk와 유사하지만, 조각의 크기를 더 세밀하게 지정할 수 있습니다. 즉, 각 조각이 몇 개의 요소를 포함할지 직접 결정할 수 있습니다. \\\n",
    "\\\n",
    "만약 여러분이 [10, 20] 크기의 Tensor를 각각 4개와 6개의 요소를 포함하는 두 조각으로 나누고 싶다면, split 함수를 사용하여 이를 수행할 수 있습니다. \\\n",
    "이렇게 하면, data를 여러분의 필요에 따라 유연하게 나눌 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eee5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 조각의 크기: torch.Size([4, 20])\n",
      "두 번째 조각의 크기: torch.Size([6, 20])\n"
     ]
    }
   ],
   "source": [
    "# [10, 20] 크기의 텐서 생성\n",
    "tensor = torch.randn(10, 20)\n",
    "\n",
    "# 첫 번째 차원(길이가 10)을 기준으로, 4개와 6개 요소를 포함하는 두 조각으로 나눔\n",
    "split_sizes = (4, 6)    # 조각의 크기를 지정\n",
    "splits = tensor.split(split_sizes, dim=0)   # 0번째 차원(행)을 기준으로 나눔\n",
    "\n",
    "# 나누어진 조각들 확인\n",
    "print(f\"첫 번째 조각의 크기: {splits[0].size()}\")\n",
    "print(f\"두 번째 조각의 크기: {splits[1].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6529607",
   "metadata": {},
   "source": [
    "# 3. Tensor Indexing과 Slicing\n",
    "## 3.1. Tensor Indexing\n",
    "Tensor Indexing은 마치 책장에서 원하는 책을 찾아내는 것과 유사합니다. 여러분이 특정 요소에 접근하고 싶을 때 사용하는 방법입니다. \\\n",
    "PyTorch에서는 대괄호([])를 사용하여 Tenosr의 특정 위치에 있는 data에 접근할 수 있습니다. \\\n",
    "\\\n",
    "예를 들어, tensor[1, 2]는 Tensor의 두 번째 행과 세 번째 열에 있는 요소를 의미합니다.\\\n",
    "이 방법을 사용하면, Tensor 내의 한 개 또는 여러 개의 특정 요소를 선택할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fa5c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "selected_element = tensor[1, 2]\n",
    "print(selected_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878d194",
   "metadata": {},
   "source": [
    "## 3.2. Tensor Slicing\n",
    "Tensor Slicing은 케이크를 여러 조각으로 나누어 친구들과 나누어 먹는 것과 비슷합니다. 즉, Tensor의 일부분을 선택적으로 접근하고 싶을 때 사용하는 방법입니다.\\\n",
    "PyTorch에서는 콜론(:)을 사용하여 시작 index, 끝 index, 그리고 step 크기를 지정함으로써 Tensor의 특정 범위를 선택할 수 있습니다.\\\n",
    "예를 들어, tensor[:, 1:4]는 Tensor의 모든 행과 두 번째 열부터 네 번째 열까지의 범위를 의미합니다. 이 방법을 통해 원하는 부분 Tensor를 쉽게 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdbe01db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텐서:\n",
      " tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19],\n",
      "        [20, 21, 22, 23, 24]])\n",
      "\n",
      "슬라이스된 텐서 (모든 행, 두 번째~네 번째 열):\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 6,  7,  8],\n",
      "        [11, 12, 13],\n",
      "        [16, 17, 18],\n",
      "        [21, 22, 23]])\n"
     ]
    }
   ],
   "source": [
    "# 0부터 시작하여 순서대로 증가하는 값을 가진 텐서 생성\n",
    "# torch.arange로 0부터 24까지의 숫자를 생성하고, 이를 [5, 5] 크기로 재구성\n",
    "tensor = torch.arange(0, 25).reshape(5, 5)\n",
    "print(\"원본 텐서:\\n\", tensor)\n",
    "\n",
    "sliced_tensor = tensor[:, 1:4]\n",
    "\n",
    "print(\"\\n슬라이스된 텐서 (모든 행, 두 번째~네 번째 열):\")\n",
    "print(sliced_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f521e76",
   "metadata": {},
   "source": [
    "## 3.3. 조건부 Indexing\n",
    "조건부 Indexing은 여러분이 특정 조건을 만족하는 요소만 선택하고 싶을 때 사용합니다. 마치 여러분이 과일을 고를 때, 달콤한 과일만 고르는 것과 비슷하다고 할 수 있습니다.\\\n",
    "PyTorch에서는 Boolean Tensor를 사용하여 이를 수행할 수 있습니다.\\\n",
    "\\\n",
    "예를 들어, tensor[tensor > 5]는 Tensor 내에서 5보다 큰 모든 요소를 선택합니다. 이 방법은 data를 filtering하거나 조건에 따라 특정 data를 추출하고 싶을 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63abce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 이상 값:\n",
      " tensor([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
      "        24])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0, 25).reshape(5, 5)\n",
    "\n",
    "mask = tensor > 5   # 조건부 인덱싱\n",
    "selected_elements = tensor[mask]\n",
    "print(\"5 이상 값:\\n\", selected_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f96b15",
   "metadata": {},
   "source": [
    "# 4. 고급 Indexing 기법\n",
    "## 4.1. gather\n",
    "gather 함수는 Tensor 내의 data를 index에 따라 모으는 연산입니다. 이 함수는 다차원 index를 사용하여 주어진 차원에 대해 Tensor의 요소를 선택적으로 추출합니다. gather는 주로 복잡한 data 추출 작업이나, 특정 조건에 맞는 data를 선택할 때 유용합니다.\\\n",
    "\\\n",
    "예를 들어 y는 [[1,2],[3,4]]라는 2x2 크기의 Tensor입니다. 여기에 [[0,0], [1,0]]를 index로 지정하면, 첫 번째 행 [0,0]은 y의 첫 번째 행에서 첫 번째 열의 값을 두 번 수집하라는 의미힙니다. 따라서 결과는 [1, 1]이 됩니다.\\\n",
    "두 번째 행 [1, 0]은 y의 두 번째 행에서 두 번째 열의 값을 첫 번째로, 그리고 첫 번째 열의 값을 두 번째로 수집하라는 의미입니다. 따라서 결과는 [4, 3]이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018d32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출한 텐서: \n",
      " tensor([[1, 1],\n",
      "        [4, 3]])\n"
     ]
    }
   ],
   "source": [
    "# gather 예제\n",
    "y = torch.tensor([[1, 2], [3, 4]])\n",
    "gather_index = torch.tensor([[0, 0], [1, 0]])\n",
    "gathered = y.gather(1, gather_index)    # 지정된 인덱스에서 값을 수집\n",
    "print(\"추출한 텐서: \\n\", gathered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70dc30",
   "metadata": {},
   "source": [
    "## 4.2. masked_select\n",
    "masked_select 함수는 Boolean mask를 사용하여 Tensor에서 요소를 선택합니다. 이 함수는 주어진 mask에 따라 True에 해당하는 위치의 요소만을 선택하여 일차원 Tensor로 반환합니다.\\\n",
    "masked_select는 조건부 선택이 필요할 때 특히 유용하며, data filtering이나 특정 조건을 만족하는 요소를 추출할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b87ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출한 텐서:\n",
      " tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# 마스크 생성\n",
    "mask = torch.tensor([True, False, True, False])\n",
    "selected = tensor.masked_select(mask)\n",
    "print(\"추출한 텐서:\\n\", selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541eeda",
   "metadata": {},
   "source": [
    "## 4.3. 정수 배열을 사용한 Indexing\n",
    "복잡한 data 선택 요구사항을 충족하기 위해 사용됩니다. 이 방식을 정수 배열 또는 Boolean 배열을 사용하여 Tensor의 특정 요소들을 비연속적으로 선택할 수 있게 해줍니다.\\\n",
    "또한 해당 방법은 선택된 요소들로 구성된 새로운 Tensor를 반환합니다. 원본 Tensor의 구조와는 독립적일 수 있습니다.\\\n",
    "\\\n",
    "자세한 내용은 아래 예시를 참고하세요.\n",
    "- rows = torch.tensor([0, 2])는 첫 번째 행과 두 번째 행을 선택하겠다는 의미입니다.\n",
    "- cols = torch.tensor([1, 2])는 두 번째 열과 세 번째 열을 선택하겠다는 의미입니다.\n",
    "- selected = tensor[rows, cols]는 tensor 텐서에서 rows와 cols에 지정된 위치의 요소들을 선택하여 새로운 텐서로 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8db2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출한 텐서:  tensor([2, 9])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 정수 배열을 사용한 인덱싱\n",
    "rows = torch.tensor([0, 2])\n",
    "cols = torch.tensor([1, 2])\n",
    "\n",
    "# 선택된 행과 열에 해당하는 요소를 추출\n",
    "selected_by_index = tensor[rows, cols]\n",
    "\n",
    "print(\"추출한 텐서: \", selected_by_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93277370",
   "metadata": {},
   "source": [
    "# Tensor와 NumPy 간의 변환\n",
    "## 5.1. NumPy 배열을 Tensor로 변환\n",
    "torch.from_numpy 함수는 NumPy 배열을 PyTorch Tensor로 변환합니다.\\\n",
    "이 함수는 memory를 공유하기 때문에, 변환된 Tensor를 변경하면 원본 NumPy 배열도 변경됩니다.\\\n",
    "이는 data 복사를 피하고 효율성을 높이기 위한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9915cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넘파이 배열:  [1 2 3 4 5]\n",
      "텐서:  tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 넘파이 배열 생성\n",
    "numpy_array = np.array([1, 2, 3, 4, 5])\n",
    "print(\"넘파이 배열: \", numpy_array)\n",
    "\n",
    "# 넘파일 배열을 텐서로 변환\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"텐서: \", tensor_from_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ae9ba",
   "metadata": {},
   "source": [
    "## 5.2. Tensor를 NumPy 배열로 변환\n",
    "tensor.numpy() 메서드는 PyTorch Tensor를 NumPy 배열로 변환합니다.\\\n",
    "이 변환 역시 memory를 공유하기 때문에, 변환된 배열을 변경하면 원본 Tensor도 변경됩니다.\\\n",
    "이 메서드는 PyTorch Tensor에 대해 호출됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9fd5f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서:  tensor([1, 2, 3, 4, 5])\n",
      "넘파이 배열:  [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# 텐서 생성\n",
    "tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"텐서: \", tensor)\n",
    "\n",
    "# 텐서를 넘파이 배열로 변환\n",
    "numpy_from_tensor = tensor.numpy()\n",
    "print(\"넘파이 배열: \", numpy_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b65376",
   "metadata": {},
   "source": [
    "## 5.3. Data 공유의 중요성\n",
    "torch.from_numpy와 tensor.numpy() 모두 원본 data와 변환된 data 사이에서 memory를 공유합니다.\\\n",
    "이는 data를 복사하지 않아도 되므로 성능상의 이점을 가집니다. 하지만 이로 인해 한쪽에서 data를 변경하면 다른 쪽에도 영향을 미치므로, 이 점을 유의하여 사용해야 합니다.\\\n",
    "data 복사가 필요한 경우에는 명시적으로 copy(복사) 연산을 수행해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ceadb2",
   "metadata": {},
   "source": [
    "# 6. Tensor 복제 - clone\n",
    "clone 함수는 PyTorch에서 Tensor의 data를 복사하여 새로운 Tensor를 생성하는 데 사용됩니다. 이 함수는 원본 Tensor의 data를 완전히 복제하여 새로운 memory 위치에 저장합니다.\\\n",
    "\\\n",
    "결과적으로, clone으로 생성된 Tensor는 원본 Tensor와 data는 같지만, 서로 독립적인 memory를 가집니다. 따라서, clone된 Tensor를 수정해도 원본 Tensor에는 영향을 주지 않습니다.\\\n",
    "\\\n",
    "사용 용도\n",
    "- data의 안전한 복제: Tensor의 data를 수정하려고 할 때, 원본 data를 보존하면서 실험하고 싶을 때 유용합니다.\n",
    "- 계산 graph와의 독립성: clone은 기본적으로 복제된 Tensor에 대한 새로운 계산 graph를 생성합니다. 이는 자동 미분에 영향을 줄 수 있는 연산에서 특히 중요할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e0f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서 원본:  tensor([1., 2., 3., 4., 5.])\n",
      "복제된 텐서:  tensor([1., 2., 3., 4., 5.])\n",
      "수정된 텐서:  tensor([10.,  2.,  3.,  4.,  5.])\n",
      "텐서 원본 확인:  tensor([1., 2., 3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 원본 텐서 생성\n",
    "original_tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "print(\"텐서 원본: \", original_tensor)\n",
    "\n",
    "# 텐서 복제\n",
    "cloned_tensor = original_tensor.clone()\n",
    "print(\"복제된 텐서: \", cloned_tensor)\n",
    "\n",
    "# 복제된 텐서 수정\n",
    "cloned_tensor[0] = 10\n",
    "print(\"수정된 텐서: \", cloned_tensor)\n",
    "\n",
    "# 원본 텐서 확인 (원본 텐서는 변경되지 않음)\n",
    "print(\"텐서 원본 확인: \", original_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
