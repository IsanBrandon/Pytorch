{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a2a0ad",
   "metadata": {},
   "source": [
    "# Stage 4. Tensor의 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272eee06",
   "metadata": {},
   "source": [
    "# 1. 기본 연산\n",
    "## 1.1. Tensor와 Scalar의 연산\n",
    "PyTorch에서 Tensor와 Scalar(단일 숫자)간의 사칙연산은 지원됩니다. Tensor와 Scalar 사이의 덧셈 연산을 수행할 때, Scalar 값은 Tensor의 모든 요소에 대해 적용됩니다. \\\n",
    "\\\n",
    "연산 방법은 Python과 동일합니다.\n",
    "- 덧셈: +\n",
    "- 뺄셈: -\n",
    "- 곱셈: *\n",
    "- 나눗셈: /\n",
    "- 제곱: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbbf02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar 덧셈: \n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "Scalar 뺄셈: \n",
      " tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "Scalar 곱셈: \n",
      " tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "Scalar 나눗셈: \n",
      " tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "Scalar 제곱: \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "\n",
    "# Tensor와 Scalar의 덧셈\n",
    "add_scalar = tensor + 1\n",
    "\n",
    "# Tensor와 Scalar의 뺄셈\n",
    "sub_scalar = tensor - 1\n",
    "\n",
    "# Tensor와 Scalar의 곱셈\n",
    "mul_scalar = tensor * 2\n",
    "\n",
    "# Tensor와 Scalar의 나눗셈\n",
    "div_scalar = tensor / 2\n",
    "\n",
    "# Tensor와 Scalar의 제곱\n",
    "pow_scalar = tensor ** 2\n",
    "\n",
    "print('Scalar 덧셈: \\n', add_scalar)\n",
    "print('Scalar 뺄셈: \\n', sub_scalar)\n",
    "print('Scalar 곱셈: \\n', mul_scalar)\n",
    "print('Scalar 나눗셈: \\n', div_scalar)\n",
    "print('Scalar 제곱: \\n', pow_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11816d",
   "metadata": {},
   "source": [
    "## 1.2. 덧셈과 뺄셈\n",
    "Tensor의 덧셈과 뺄셈 연산은 두 Tensor의 동일 위치에 있는 요소들 끼리 연산을 수행합니다.\\\n",
    "\\\n",
    "예를 들어, 두 개의 2x2 텐서 tensor_a와 tensor_b는 각각 [[1,2],[3,4]]와 [[5,6],[7,8]]의 값을 갖을 때, 이 둘의 연산은 아래와 같이 계산됩니다.\n",
    "- 덧셈 결과: 각 요소를 더한 결과인 [[6,8],[10,12]]를 얻습니다. 이는 tensor_a의 각 요소에 tensor_b의 해당 요소를 더하여 계산된 결과입니다.\n",
    "- 뺄셈 결과: tensor_a에서 tensor_b를 뺀 결과인 [[-4,-4],[-4,-4]]를 얻습니다. 이는 tensor_a의 각 요소에서 tensor_b의 해당 요소를 빼서 계산된 결과입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d6ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "덧셈 결과: \n",
      " tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "뺄셈 결과: \n",
      " tensor([[-4, -4],\n",
      "        [-4, -4]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1,2], [3,4]])\n",
    "tensor_b = torch.tensor([[5,6], [7,8]])\n",
    "\n",
    "# Tensor 덧셈\n",
    "add_result = tensor_a + tensor_b\n",
    "# 또는 torch.add(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# Tensor 뺄셈\n",
    "sub_result = tensor_a - tensor_b\n",
    "# 또는 torch.sub(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('덧셈 결과: \\n', add_result)\n",
    "print('뺄셈 결과: \\n', sub_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a6cd1",
   "metadata": {},
   "source": [
    "## 1.3. 곱셈과 나눗셈\n",
    "요소별 곱셈과 나눗셈 연산도 마찬가지로 각 텐서의 동일한 위치에 있는 요소끼리 수행됩니다.\\\n",
    "\\\n",
    "예를 들어, tensor_a와 tensor_b는 각각 [2,3,4]와 [5,6,7]의 값을 갖는 Tensor입니다. 연산 결과는 다음과 같습니다.\n",
    "- 요소별 곱셈 결과: 각 요소의 곱셈 결과인 [10, 18, 28]을 얻습니다.\n",
    "- 요소별 나눗셈 결과: 각 요소를 나눈 결과인 [0.4, 0.5, 0.5714...]를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26b532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요소별 곱셈:  tensor([10., 18., 28.])\n",
      "요소별 나눗셈:  tensor([0.4000, 0.5000, 0.5714])\n"
     ]
    }
   ],
   "source": [
    "# 두 Tensor 생성\n",
    "tensor_a = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
    "tensor_b = torch.tensor([5, 6, 7], dtype=torch.float32)\n",
    "\n",
    "# 요소별 곱셈\n",
    "product = tensor_a * tensor_b\n",
    "# 또는 torch.mul(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "print('요소별 곱셈: ', product)\n",
    "\n",
    "# 요소별 나눗셈\n",
    "division = tensor_a / tensor_b\n",
    "# 또는 torch.div(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "print('요소별 나눗셈: ', division)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141c460",
   "metadata": {},
   "source": [
    "## 1.4. Broadcasting 이해하기\n",
    "Broadcasting을 통해 서로 다른 shape을 가진 Tensor들 간에도 수학 연산을 수행할 수 있게 됩니다. 이는 더 작은 Tensor가 큰 Tensor의 모양에 맞게 자동으로 확장(확대)되어 연산이 가능하게 만듭니다. \\\n",
    "\\\n",
    "Broadcasting이 작동하는 방법은 다음과 같은 규칙을 따릅니다.\n",
    "1. 차원의 크기가 같거나, 하나의 차원이 1인 경우에만 Broadcasting이 가능합니다. 예를 들어, (5, 4) 모양의 Tensor와 (1, 4) 모양의 Tensor는 Broadcasting이 가능합니다. 여기서 (1, 4) 모양의 Tensor는 첫 번째 차원을 따라 5회 반복되어 (5, 4) 모양으로 확장됩니다.\n",
    "2. Tensor의 차원 수가 다를 경우, 더 작은 차원을 가진 Tensor의 모양 앞에 1을 추가하여 차원의 수를 맞춥니다. 예를 들어, (5, 4) 모양의 Tensor와 (4,) 모양의 Tensor가 있을 때, 더 작은 Tensor는 (1, 4)로 간주됩니다. 그 후, 앞서 설명한 바와 같이 Broadcasting이 수행됩니다.\n",
    "3. Broadcasting은 각 차원을 따라 반복함으로써 더 큰 모양의 Tensor에 맞추어 확장합니다. 이 과정은 실제 data 복사가 일어나지 않으며, 연산을 효율적으로 만들기 위한 가상의 확장으로 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1c5d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요소별 덧셈 결과: \n",
      " tensor([[5., 7., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 (1, 3)인 Tensor 생성\n",
    "tensor_a = torch.tensor([[1, 2, 3]], dtype=torch.float32)  \n",
    "\n",
    "# 크기가 (3,)인 Tensor 생성\n",
    "tensor_b = torch.tensor([4, 5, 6], dtype=torch.float32)   \n",
    "\n",
    "# tensor_a와 tensor_b의 요소별 덧셈 (Broadcasting 발생)\n",
    "# tensor_b가 tensor_a의 모양에 맞게 확장되어 연산됨\n",
    "result_add = tensor_a + tensor_b\n",
    "print('요소별 덧셈 결과: \\n', result_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1051302",
   "metadata": {},
   "source": [
    "# 2. 비교 연산\n",
    "## 2.1. 동등 비교\n",
    "동등 비교는 두 Tensor 간의 요소별 동등성을 비교하는 데 사용하는 것입니다. \\\n",
    "\\\n",
    "동등비교를 위해 ==또는 torch.eq()를 사용합니다. 이 함수는 두 Tensor의 동일한 위치에 있는 요소가 같은지 여부를 검사하고, 결과를 Boolean Tensor로 반환합니다. 즉, 두 요소가 같으면 True, 다르면 False 값을 갖습니다. \\\n",
    "\\\n",
    "torch.eq() 함수의 사용법은 매우 간단합니다. 두 Tensor a와 b가 주어졌을 때, torch.eq(a, b)를 호출하여 두 Tensor 간의 요소별 동등 비교를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c05cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동등 비교 결과:  tensor([ True,  True, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([1, 2, 3, 4, 5])\n",
    "tensor_b = torch.tensor([1, 2, 0, 4, 5])\n",
    "\n",
    "result = tensor_a == tensor_b\n",
    "# torch.eq(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('동등 비교 결과: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732cb79",
   "metadata": {},
   "source": [
    "## 2.2. 대소 비교(1) - 크다/작다\n",
    "PyTorch에서 Tensor 간의 대소 비교는 각 요소의 값을 비교하여 해당 조건이 참인지 거짓인지를 판단하는 연산입니다.\\\n",
    "\\\n",
    "대소 비교에는 아래와 같은 함수를 이용합니다.\n",
    "- \\>, torch.gt(a, b): a가 b보다 큰 경우 True를 반환합니다. (greater than)\n",
    "- <, torch.lt(a, b): a가 b보다 작은 경우 True를 반환합니다. (less than)\n",
    "- \\>=, torch.ge(a, b): a가 b보다 크거나 같은 경우 True를 반환합니다. (greater than or equal to)\n",
    "- <=, torch.le(a, b): a가 b보다 작거나 같은 경우 True를 반환합니다. (less than or equal to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa45881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a가 b보다 큰가:  tensor([ True, False, False, False])\n",
      "a가 b보다 작은가:  tensor([False, False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([5, 6, 7, 8])\n",
    "tensor_b = torch.tensor([4, 6, 7, 10])\n",
    "\n",
    "# 조건: a 텐서가 b 텐서보다 크다\n",
    "gt_result = tensor_a > tensor_b\n",
    "# 또는 torch.gt(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# 조건: a 텐서가 b 텐서보다 작다\n",
    "lt_result = tensor_a < tensor_b\n",
    "# 또는 torch.lt(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('a가 b보다 큰가: ', gt_result)\n",
    "print('a가 b보다 작은가: ', lt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f5cbf",
   "metadata": {},
   "source": [
    "## 2.3. 대소 비교(2) - 같거나 크다/작다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242d831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a가 b보다 크거나 같은가:  tensor([ True,  True,  True, False])\n",
      "a가 b보다 작거나 같은가:  tensor([False,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "# Tensor 간 요소별 대소 비교 (a >= b)\n",
    "ge_result = tensor_a >= tensor_b\n",
    "# 또는 torch.ge(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# Tensor 간 요소별 대소 비교 (a <= b)\n",
    "le_result = tensor_a <= tensor_b\n",
    "# 또는 torch.le(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('a가 b보다 크거나 같은가: ', ge_result)\n",
    "print('a가 b보다 작거나 같은가: ', le_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e408db4",
   "metadata": {},
   "source": [
    "## 2.4. 조건을 만족하는 요소 선택하기\n",
    "대소 비교 연산을 통해 생성된 Boolean Tensor(mask)를 사용하여 조건을 만족하는 data를 추출할 수 있어요. 이 방법은 data filtering, 선별적 연산 수행 등 다양한 상황에서 활용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c523cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조건을 만족하는 A의 요소:  tensor([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 두 Tensor 생성\n",
    "A = torch.tensor([1, 4, 3, 2, 5])\n",
    "B = torch.tensor([3, 2, 1, 5, 4])\n",
    "\n",
    "# A의 각 요소가 B의 해당 요소보다 큰지 비교\n",
    "mask = torch.gt(A, B)\n",
    "# 또는 mask = A > B 를 사용할 수 있습니다.\n",
    "\n",
    "# 조건을 만족하는 A의 요소 선택\n",
    "selected_elements = A[mask]\n",
    "\n",
    "print('조건을 만족하는 A의 요소: ', selected_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502b400",
   "metadata": {},
   "source": [
    "# 3. 축소 연산\n",
    "## 3.1. 최대값/최소값 구하기\n",
    "- 최대값: Tenosr 내의 최대값을 구하기 위해 torch.max()함수를 사용합니다. 이 함수는 Tensor 전체의 최대값을 반환할 수도 있고, 특정 차원을 기준으로 각 부분의 최대값과 그 indices(위치)를 반환할 수도 있습니다.\n",
    "- 최소값: Tensor 내의 최소값을 구하기 위해 torch.min()함수를 사용합니다. 마찬가지로, Tenosr 전체의 최소값을 단일 값으로 반환하거나, 특정 차원에 따른 최소값과 해당 위치를 반환할 수 있습니다.\n",
    "\n",
    "함수에 dim 매개변수를 지정함으로써, 특정 차원을 기준으로 한 최대값 최소값을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57d055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 Tensor의 최대값:  tensor(9)\n",
      "전체 Tensor의 최소값:  tensor(1)\n",
      "각 행의 최대값:  tensor([3, 6, 9])\n",
      "각 행의 최대값 위치:  tensor([2, 2, 2])\n",
      "각 행의 최소값:  tensor([1, 4, 7])\n",
      "각 행의 최소값 위치:  tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 전체 Tenosr에서 최대값 구하기\n",
    "max_value = torch.max(tensor)\n",
    "print('전체 Tensor의 최대값: ', max_value)\n",
    "\n",
    "# 전체 Tensor에서 최소값 구하기\n",
    "min_value = torch.min(tensor)\n",
    "print('전체 Tensor의 최소값: ', min_value)\n",
    "\n",
    "# 특정 차원을 기준으로 최대값과 그 위치 구하기\n",
    "max_values, max_indices = torch.max(tensor, dim=1)\n",
    "print('각 행의 최대값: ', max_values)\n",
    "print('각 행의 최대값 위치: ', max_indices)\n",
    "\n",
    "# 특정 차원을 기준으로 최소값과 그 위치 구하기\n",
    "min_values, min_indices = torch.min(tensor, dim=1)\n",
    "print('각 행의 최소값: ', min_values)\n",
    "print('각 행의 최소값 위치: ', min_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ed8b4",
   "metadata": {},
   "source": [
    "## 3.2. 합계\n",
    "Tensor 전체 요소의 합계를 구하기 위해 torch.sum()함수를 사용합니다. 이 함수는 Tensor 내 모든 요소의 합을 반환합니다.\\\n",
    "\\\n",
    "torch.sum()함수에 dim 매개변수를 지정함으로써, 특정 차원을 기준으로 한 합계를 계산할 수 있습니다. 예를 들어, 행 또는 열의 합계를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f2f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 전체의 합계:  tensor(45)\n",
      "각 열의 합계:  tensor([12, 15, 18])\n",
      "각 행의 합계:  tensor([ 6, 15, 24])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 전체 Tensor의 합계 구하기\n",
    "total_sum = torch.sum(tensor)\n",
    "print('Tensor 전체의 합계: ', total_sum)\n",
    "\n",
    "# 각 열의 합계 구하기 (열 방향)\n",
    "col_sum = torch.sum(tensor, dim=0)\n",
    "print('각 열의 합계: ', col_sum)\n",
    "\n",
    "# 각 행의 합계 구하기 (행 방향)\n",
    "row_sum = torch.sum(tensor, dim=1)\n",
    "print('각 행의 합계: ', row_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255b569",
   "metadata": {},
   "source": [
    "## 3.3 평균/중앙값/최빈값\n",
    "- 평균: data 집합의 총합을 data의 개수로 나눈 값입니다. torch.mean()함수를 사용하여 Tensor의 평균을 구할 수 있습니다. 이 함수는 부동 수소점 type의 Tensor에 대해서만 작동하므로, 정수 type tensor의 평균을 구하려면 type 변환을 해야 합니다.\n",
    "\n",
    "- 중앙값: data를 크기 순으로 나열했을 때 중앙에 위치하는 값입니다. torch.median()함수를 사용하여 Tensor의 중앙값을 구할 수 있습니다.\n",
    "\n",
    "- 최빈값: data 집합에서 가장 자주 등장하는 값을 의미합니다. torch.mode()함수를 사용하여 Tensor의 최빈값과 그 index를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d07451",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
