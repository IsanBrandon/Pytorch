{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a2a0ad",
   "metadata": {},
   "source": [
    "# Stage 4. Tensor의 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272eee06",
   "metadata": {},
   "source": [
    "# 1. 기본 연산\n",
    "## 1.1. Tensor와 Scalar의 연산\n",
    "PyTorch에서 Tensor와 Scalar(단일 숫자)간의 사칙연산은 지원됩니다. Tensor와 Scalar 사이의 덧셈 연산을 수행할 때, Scalar 값은 Tensor의 모든 요소에 대해 적용됩니다. \\\n",
    "\\\n",
    "연산 방법은 Python과 동일합니다.\n",
    "- 덧셈: +\n",
    "- 뺄셈: -\n",
    "- 곱셈: *\n",
    "- 나눗셈: /\n",
    "- 제곱: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bbbf02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar 덧셈: \n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "Scalar 뺄셈: \n",
      " tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "Scalar 곱셈: \n",
      " tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "Scalar 나눗셈: \n",
      " tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "Scalar 제곱: \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "\n",
    "# Tensor와 Scalar의 덧셈\n",
    "add_scalar = tensor + 1\n",
    "\n",
    "# Tensor와 Scalar의 뺄셈\n",
    "sub_scalar = tensor - 1\n",
    "\n",
    "# Tensor와 Scalar의 곱셈\n",
    "mul_scalar = tensor * 2\n",
    "\n",
    "# Tensor와 Scalar의 나눗셈\n",
    "div_scalar = tensor / 2\n",
    "\n",
    "# Tensor와 Scalar의 제곱\n",
    "pow_scalar = tensor ** 2\n",
    "\n",
    "print('Scalar 덧셈: \\n', add_scalar)\n",
    "print('Scalar 뺄셈: \\n', sub_scalar)\n",
    "print('Scalar 곱셈: \\n', mul_scalar)\n",
    "print('Scalar 나눗셈: \\n', div_scalar)\n",
    "print('Scalar 제곱: \\n', pow_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11816d",
   "metadata": {},
   "source": [
    "## 1.2. 덧셈과 뺄셈\n",
    "Tensor의 덧셈과 뺄셈 연산은 두 Tensor의 동일 위치에 있는 요소들 끼리 연산을 수행합니다.\\\n",
    "\\\n",
    "예를 들어, 두 개의 2x2 텐서 tensor_a와 tensor_b는 각각 [[1,2],[3,4]]와 [[5,6],[7,8]]의 값을 갖을 때, 이 둘의 연산은 아래와 같이 계산됩니다.\n",
    "- 덧셈 결과: 각 요소를 더한 결과인 [[6,8],[10,12]]를 얻습니다. 이는 tensor_a의 각 요소에 tensor_b의 해당 요소를 더하여 계산된 결과입니다.\n",
    "- 뺄셈 결과: tensor_a에서 tensor_b를 뺀 결과인 [[-4,-4],[-4,-4]]를 얻습니다. 이는 tensor_a의 각 요소에서 tensor_b의 해당 요소를 빼서 계산된 결과입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d6ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "덧셈 결과: \n",
      " tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "뺄셈 결과: \n",
      " tensor([[-4, -4],\n",
      "        [-4, -4]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1,2], [3,4]])\n",
    "tensor_b = torch.tensor([[5,6], [7,8]])\n",
    "\n",
    "# Tensor 덧셈\n",
    "add_result = tensor_a + tensor_b\n",
    "# 또는 torch.add(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# Tensor 뺄셈\n",
    "sub_result = tensor_a - tensor_b\n",
    "# 또는 torch.sub(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('덧셈 결과: \\n', add_result)\n",
    "print('뺄셈 결과: \\n', sub_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a6cd1",
   "metadata": {},
   "source": [
    "## 1.3. 곱셈과 나눗셈\n",
    "요소별 곱셈과 나눗셈 연산도 마찬가지로 각 텐서의 동일한 위치에 있는 요소끼리 수행됩니다.\\\n",
    "\\\n",
    "예를 들어, tensor_a와 tensor_b는 각각 [2,3,4]와 [5,6,7]의 값을 갖는 Tensor입니다. 연산 결과는 다음과 같습니다.\n",
    "- 요소별 곱셈 결과: 각 요소의 곱셈 결과인 [10, 18, 28]을 얻습니다.\n",
    "- 요소별 나눗셈 결과: 각 요소를 나눈 결과인 [0.4, 0.5, 0.5714...]를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26b532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요소별 곱셈:  tensor([10., 18., 28.])\n",
      "요소별 나눗셈:  tensor([0.4000, 0.5000, 0.5714])\n"
     ]
    }
   ],
   "source": [
    "# 두 Tensor 생성\n",
    "tensor_a = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
    "tensor_b = torch.tensor([5, 6, 7], dtype=torch.float32)\n",
    "\n",
    "# 요소별 곱셈\n",
    "product = tensor_a * tensor_b\n",
    "# 또는 torch.mul(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "print('요소별 곱셈: ', product)\n",
    "\n",
    "# 요소별 나눗셈\n",
    "division = tensor_a / tensor_b\n",
    "# 또는 torch.div(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "print('요소별 나눗셈: ', division)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141c460",
   "metadata": {},
   "source": [
    "## 1.4. Broadcasting 이해하기\n",
    "Broadcasting을 통해 서로 다른 shape을 가진 Tensor들 간에도 수학 연산을 수행할 수 있게 됩니다. 이는 더 작은 Tensor가 큰 Tensor의 모양에 맞게 자동으로 확장(확대)되어 연산이 가능하게 만듭니다. \\\n",
    "\\\n",
    "Broadcasting이 작동하는 방법은 다음과 같은 규칙을 따릅니다.\n",
    "1. 차원의 크기가 같거나, 하나의 차원이 1인 경우에만 Broadcasting이 가능합니다. 예를 들어, (5, 4) 모양의 Tensor와 (1, 4) 모양의 Tensor는 Broadcasting이 가능합니다. 여기서 (1, 4) 모양의 Tensor는 첫 번째 차원을 따라 5회 반복되어 (5, 4) 모양으로 확장됩니다.\n",
    "2. Tensor의 차원 수가 다를 경우, 더 작은 차원을 가진 Tensor의 모양 앞에 1을 추가하여 차원의 수를 맞춥니다. 예를 들어, (5, 4) 모양의 Tensor와 (4,) 모양의 Tensor가 있을 때, 더 작은 Tensor는 (1, 4)로 간주됩니다. 그 후, 앞서 설명한 바와 같이 Broadcasting이 수행됩니다.\n",
    "3. Broadcasting은 각 차원을 따라 반복함으로써 더 큰 모양의 Tensor에 맞추어 확장합니다. 이 과정은 실제 data 복사가 일어나지 않으며, 연산을 효율적으로 만들기 위한 가상의 확장으로 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1c5d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요소별 덧셈 결과: \n",
      " tensor([[5., 7., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 (1, 3)인 Tensor 생성\n",
    "tensor_a = torch.tensor([[1, 2, 3]], dtype=torch.float32)  \n",
    "\n",
    "# 크기가 (3,)인 Tensor 생성\n",
    "tensor_b = torch.tensor([4, 5, 6], dtype=torch.float32)   \n",
    "\n",
    "# tensor_a와 tensor_b의 요소별 덧셈 (Broadcasting 발생)\n",
    "# tensor_b가 tensor_a의 모양에 맞게 확장되어 연산됨\n",
    "result_add = tensor_a + tensor_b\n",
    "print('요소별 덧셈 결과: \\n', result_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1051302",
   "metadata": {},
   "source": [
    "# 2. 비교 연산\n",
    "## 2.1. 동등 비교\n",
    "동등 비교는 두 Tensor 간의 요소별 동등성을 비교하는 데 사용하는 것입니다. \\\n",
    "\\\n",
    "동등비교를 위해 ==또는 torch.eq()를 사용합니다. 이 함수는 두 Tensor의 동일한 위치에 있는 요소가 같은지 여부를 검사하고, 결과를 Boolean Tensor로 반환합니다. 즉, 두 요소가 같으면 True, 다르면 False 값을 갖습니다. \\\n",
    "\\\n",
    "torch.eq() 함수의 사용법은 매우 간단합니다. 두 Tensor a와 b가 주어졌을 때, torch.eq(a, b)를 호출하여 두 Tensor 간의 요소별 동등 비교를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c05cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동등 비교 결과:  tensor([ True,  True, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([1, 2, 3, 4, 5])\n",
    "tensor_b = torch.tensor([1, 2, 0, 4, 5])\n",
    "\n",
    "result = tensor_a == tensor_b\n",
    "# torch.eq(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('동등 비교 결과: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732cb79",
   "metadata": {},
   "source": [
    "## 2.2. 대소 비교(1) - 크다/작다\n",
    "PyTorch에서 Tensor 간의 대소 비교는 각 요소의 값을 비교하여 해당 조건이 참인지 거짓인지를 판단하는 연산입니다.\\\n",
    "\\\n",
    "대소 비교에는 아래와 같은 함수를 이용합니다.\n",
    "- \\>, torch.gt(a, b): a가 b보다 큰 경우 True를 반환합니다. (greater than)\n",
    "- <, torch.lt(a, b): a가 b보다 작은 경우 True를 반환합니다. (less than)\n",
    "- \\>=, torch.ge(a, b): a가 b보다 크거나 같은 경우 True를 반환합니다. (greater than or equal to)\n",
    "- <=, torch.le(a, b): a가 b보다 작거나 같은 경우 True를 반환합니다. (less than or equal to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa45881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a가 b보다 큰가:  tensor([ True, False, False, False])\n",
      "a가 b보다 작은가:  tensor([False, False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([5, 6, 7, 8])\n",
    "tensor_b = torch.tensor([4, 6, 7, 10])\n",
    "\n",
    "# 조건: a 텐서가 b 텐서보다 크다\n",
    "gt_result = tensor_a > tensor_b\n",
    "# 또는 torch.gt(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# 조건: a 텐서가 b 텐서보다 작다\n",
    "lt_result = tensor_a < tensor_b\n",
    "# 또는 torch.lt(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('a가 b보다 큰가: ', gt_result)\n",
    "print('a가 b보다 작은가: ', lt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f5cbf",
   "metadata": {},
   "source": [
    "## 2.3. 대소 비교(2) - 같거나 크다/작다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242d831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a가 b보다 크거나 같은가:  tensor([ True,  True,  True, False])\n",
      "a가 b보다 작거나 같은가:  tensor([False,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "# Tensor 간 요소별 대소 비교 (a >= b)\n",
    "ge_result = tensor_a >= tensor_b\n",
    "# 또는 torch.ge(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# Tensor 간 요소별 대소 비교 (a <= b)\n",
    "le_result = tensor_a <= tensor_b\n",
    "# 또는 torch.le(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('a가 b보다 크거나 같은가: ', ge_result)\n",
    "print('a가 b보다 작거나 같은가: ', le_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e408db4",
   "metadata": {},
   "source": [
    "## 2.4. 조건을 만족하는 요소 선택하기\n",
    "대소 비교 연산을 통해 생성된 Boolean Tensor(mask)를 사용하여 조건을 만족하는 data를 추출할 수 있어요. 이 방법은 data filtering, 선별적 연산 수행 등 다양한 상황에서 활용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c523cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조건을 만족하는 A의 요소:  tensor([4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 두 Tensor 생성\n",
    "A = torch.tensor([1, 4, 3, 2, 5])\n",
    "B = torch.tensor([3, 2, 1, 5, 4])\n",
    "\n",
    "# A의 각 요소가 B의 해당 요소보다 큰지 비교\n",
    "mask = torch.gt(A, B)\n",
    "# 또는 mask = A > B 를 사용할 수 있습니다.\n",
    "\n",
    "# 조건을 만족하는 A의 요소 선택\n",
    "selected_elements = A[mask]\n",
    "\n",
    "print('조건을 만족하는 A의 요소: ', selected_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502b400",
   "metadata": {},
   "source": [
    "# 3. 축소 연산\n",
    "## 3.1. 최대값/최소값 구하기\n",
    "- 최대값: Tenosr 내의 최대값을 구하기 위해 torch.max()함수를 사용합니다. 이 함수는 Tensor 전체의 최대값을 반환할 수도 있고, 특정 차원을 기준으로 각 부분의 최대값과 그 indices(위치)를 반환할 수도 있습니다.\n",
    "- 최소값: Tensor 내의 최소값을 구하기 위해 torch.min()함수를 사용합니다. 마찬가지로, Tenosr 전체의 최소값을 단일 값으로 반환하거나, 특정 차원에 따른 최소값과 해당 위치를 반환할 수 있습니다.\n",
    "\n",
    "함수에 dim 매개변수를 지정함으로써, 특정 차원을 기준으로 한 최대값 최소값을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57d055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 Tensor의 최대값:  tensor(9)\n",
      "전체 Tensor의 최소값:  tensor(1)\n",
      "각 행의 최대값:  tensor([3, 6, 9])\n",
      "각 행의 최대값 위치:  tensor([2, 2, 2])\n",
      "각 행의 최소값:  tensor([1, 4, 7])\n",
      "각 행의 최소값 위치:  tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 전체 Tenosr에서 최대값 구하기\n",
    "max_value = torch.max(tensor)\n",
    "print('전체 Tensor의 최대값: ', max_value)\n",
    "\n",
    "# 전체 Tensor에서 최소값 구하기\n",
    "min_value = torch.min(tensor)\n",
    "print('전체 Tensor의 최소값: ', min_value)\n",
    "\n",
    "# 특정 차원을 기준으로 최대값과 그 위치 구하기\n",
    "max_values, max_indices = torch.max(tensor, dim=1)\n",
    "print('각 행의 최대값: ', max_values)\n",
    "print('각 행의 최대값 위치: ', max_indices)\n",
    "\n",
    "# 특정 차원을 기준으로 최소값과 그 위치 구하기\n",
    "min_values, min_indices = torch.min(tensor, dim=1)\n",
    "print('각 행의 최소값: ', min_values)\n",
    "print('각 행의 최소값 위치: ', min_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ed8b4",
   "metadata": {},
   "source": [
    "## 3.2. 합계\n",
    "Tensor 전체 요소의 합계를 구하기 위해 torch.sum()함수를 사용합니다. 이 함수는 Tensor 내 모든 요소의 합을 반환합니다.\\\n",
    "\\\n",
    "torch.sum()함수에 dim 매개변수를 지정함으로써, 특정 차원을 기준으로 한 합계를 계산할 수 있습니다. 예를 들어, 행 또는 열의 합계를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f2f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 전체의 합계:  tensor(45)\n",
      "각 열의 합계:  tensor([12, 15, 18])\n",
      "각 행의 합계:  tensor([ 6, 15, 24])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 전체 Tensor의 합계 구하기\n",
    "total_sum = torch.sum(tensor)\n",
    "print('Tensor 전체의 합계: ', total_sum)\n",
    "\n",
    "# 각 열의 합계 구하기 (열 방향)\n",
    "col_sum = torch.sum(tensor, dim=0)\n",
    "print('각 열의 합계: ', col_sum)\n",
    "\n",
    "# 각 행의 합계 구하기 (행 방향)\n",
    "row_sum = torch.sum(tensor, dim=1)\n",
    "print('각 행의 합계: ', row_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255b569",
   "metadata": {},
   "source": [
    "## 3.3 평균/중앙값/최빈값\n",
    "- 평균: data 집합의 총합을 data의 개수로 나눈 값입니다. torch.mean()함수를 사용하여 Tensor의 평균을 구할 수 있습니다. 이 함수는 부동 수소점 type의 Tensor에 대해서만 작동하므로, 정수 type tensor의 평균을 구하려면 type 변환을 해야 합니다.\n",
    "\n",
    "- 중앙값: data를 크기 순으로 나열했을 때 중앙에 위치하는 값입니다. torch.median()함수를 사용하여 Tensor의 중앙값을 구할 수 있습니다.\n",
    "\n",
    "- 최빈값: data 집합에서 가장 자주 등장하는 값을 의미합니다. torch.mode()함수를 사용하여 Tensor의 최빈값과 그 index를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e300be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균값:  tensor(5.)\n",
      "중앙값:  tensor(5.)\n",
      "최빈값:  tensor(1.)\n",
      "최빈값의 인덱스:  tensor(0)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n",
    "\n",
    "# 평균 구하기\n",
    "mean_value = torch.mean(tensor)\n",
    "print('평균값: ', mean_value)\n",
    "\n",
    "# 중앙값 구하기\n",
    "median_value = torch.median(tensor)\n",
    "print('중앙값: ', median_value)\n",
    "\n",
    "# 최빈값 구하기\n",
    "mode_value, mode_index = torch.mode(tensor)\n",
    "print('최빈값: ', mode_value)\n",
    "print('최빈값의 인덱스: ', mode_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d07451",
   "metadata": {},
   "source": [
    "## 3.4. 분산/표준편차\n",
    "Variance(분산)과 Standard Deviation(표준편차)는 data의 분포가 평균으로부터 얼마나 떨어져 있는지를 나타내는 중요한 통계량입니다. 이들은 data의 변동성을 측정하는 데 사용되며, data가 평균값을 중심으로 얼마나 퍼져 있는지의 정도를 나타냅니다.\n",
    "- **분산**: data와 평균 간의 차이의 제곱의 평균입니다. PyTorch에서 torch.var()함수를 사용하여 Tensor의 분산을 구할 수 있습니다. 분산은 data가 평균으로부터 얼마나 멀리 떨어져 있는지의 평균적인 제곱 거리를 제공합니다.\n",
    "\n",
    "- **표준편차**: 분산의 제곱근으로, data의 분포가 평균으로부터 얼마나 퍼져 있는지를 나타내는 값입니다. torch.std()함수를 사용하여 Tensor의 표준편차를 구할 수 있습니다. 표준편차는 분산과 마찬가지로 변동성의 척도로 사용되지만, 원래 data와 같은 단위로 측정되어 이해하기 쉽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3bd43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분산:  tensor(2.5000)\n",
      "표준편차:  tensor(1.5811)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 Tensor 생성\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float32)\n",
    "\n",
    "# 분산 구하기\n",
    "variance = torch.var(tensor)\n",
    "print('분산: ', variance)\n",
    "\n",
    "# 표준편차 구하기\n",
    "standard_deviation = torch.std(tensor)\n",
    "print('표준편차: ', standard_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2308389",
   "metadata": {},
   "source": [
    "# 4. 논리 연산\n",
    "## 4.1. AND 연산\n",
    "**AND 연산**은 두 Tensor의 동일한 위치에 있는 요소들끼리 비교하여, 둘 다 True일 경우에만 결과로 True를 반환합니다. 반면에 둘 중 하나라도 True가 아닌 경우에는 False를 반환합니다.\\\n",
    "\\\n",
    "이를 위해 & 또는, torch.logical_and() 함수가 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fed01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND 연산 결과:  tensor([ True, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([True, False, True, False], dtype=torch.bool)\n",
    "tensor_b = torch.tensor([True, True, False, False], dtype=torch.bool)\n",
    "\n",
    "# 요소별 AND 연산 수행\n",
    "result_and = tensor_a & tensor_b\n",
    "# 또는 torch.logical_and(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('AND 연산 결과: ', result_and)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412de72",
   "metadata": {},
   "source": [
    "## 4.2. OR 연산\n",
    "**OR 연산**은 두 Tensor의 동일한 위치에 있는 요소들 중 하나라도 True일 경우 결과로 True를 반환합니다. 반면, 요소들 모두 False일 경우 False를 반환합니다.\\\n",
    "\\\n",
    "이를 위해 | 또는, torch.logical_or() 함수가 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7c827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR 연산 결과:  tensor([ True,  True,  True, False])\n"
     ]
    }
   ],
   "source": [
    "# 요소별 OR 연산 수행\n",
    "result_or = tensor_a | tensor_b\n",
    "# 또는 torch.logical_or(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('OR 연산 결과: ', result_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99181920",
   "metadata": {},
   "source": [
    "## 4.3. NOT 연산\n",
    "**NOT 연산**은 주어진 Boolean Tensor의 각 요소에 대해 True는 False로, False는 True로 반전시킵니다.\\\n",
    "\\\n",
    "이를 위해 ~ 또는, torch.logical_not() 함수가 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de48c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT 연산 결과:  tensor([False,  True, False,  True])\n"
     ]
    }
   ],
   "source": [
    "# 요소별 NOT 연산 수행\n",
    "result_not = ~tensor_a\n",
    "# 또는 torch.logical_not(tensor_a)를 사용할 수 있습니다.\n",
    "\n",
    "print('NOT 연산 결과: ', result_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef6660",
   "metadata": {},
   "source": [
    "# 5. 수학 연산\n",
    "## 5.1. 지수/로그/제곱근\n",
    "PyTorch에서는 수학적 연산을 위한 다양한 함수를 제공합니다. 이 중 지수, 로그, 제곱근 연산은 데이터 전처리, 특성 변환, 수치 계산 등 다양한 상황에서 널리 사용됩니다. 각 연산의 기본적인 사용 방법을 살펴봅시다.\n",
    "- **지수 연산**: 주어진 Tensor의 각 요소에 대해 밑에 e인 지수 함수를 적용합니다. torch.exp(input) 함수를 사용합니다.\n",
    "\n",
    "- **로그 연산**: 주어진 Tensor의 각 요소에 대해 밑이 e인 자연로그를 적용합니다. torch.log(input) 함수를 사용합니다. 로그 연산은 값의 scale을 조정할 때 유용하게 사용됩니다.\n",
    "\n",
    "- **제곱근 연산**: 주어진 Tensor의 각 요소에 대해 제곱근을 계산합니다. torch.sqrt(input) 함수를 사용합니다. 제곱근 연산은 분산, 표준편차 계산 등에 활용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e5f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지수 연산 결과:  tensor([ 2.7183,  7.3891, 20.0855, 54.5981])\n",
      "로그 연산 결과:  tensor([0.0000, 0.6931, 1.0986, 1.3863])\n",
      "제곱근 연산 결과:  tensor([1.0000, 1.4142, 1.7321, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "# 임의의 Tensor 생성\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# 지수 연산\n",
    "exp_result = torch.exp(tensor)\n",
    "print('지수 연산 결과: ', exp_result)\n",
    "\n",
    "# 로그 연산\n",
    "log_result = torch.log(tensor)\n",
    "print('로그 연산 결과: ', log_result)\n",
    "\n",
    "# 제곱근 연산\n",
    "sqrt_result = torch.sqrt(tensor)\n",
    "print('제곱근 연산 결과: ', sqrt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89346d7b",
   "metadata": {},
   "source": [
    "## 5.2. 제곱/절대값/역수/음수\n",
    "- **제곱 연산**: Tensor의 각 요소를 제곱합니다. torch.pow(input, exponent) 함수를 사용하며, exponent에 2를 지정할 수 있습니다. 또는 input**2 형태로도 가능합니다.\n",
    "\n",
    "- **절댓값 연산**: Tensor의 각 요소에 대한 절대값을 계산합니다. torch.abs(input) 함수를 사용합니다.\n",
    "\n",
    "- **역수 연산**: Tensor의 각 요소에 대해 역수(1을 요소값으로 나눈 값)를 계산합니다. torch.reciprocal(input) 함수를 사용합니다.\n",
    "\n",
    "- **음수 연산**: Tensor의 각 요소에 대해 음수 값을 계산합니다. torch.neg(input) 또는 -input을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "635c4728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제곱 연산 결과:  tensor([ 1.,  4.,  9., 16.])\n",
      "절대값 연산 결과:  tensor([1., 2., 3., 4.])\n",
      "역수 연산 결과:  tensor([ 1.0000, -0.5000,  0.3333, -0.2500])\n",
      "음수 연산 결과:  tensor([-1.,  2., -3.,  4.])\n"
     ]
    }
   ],
   "source": [
    "# 임의의 Tensor 생성\n",
    "tensor = torch.tensor([1.0, -2.0, 3.0, -4.0])\n",
    "\n",
    "# 제곱 연산\n",
    "squared = torch.pow(tensor, exponent=2)\n",
    "# 또는 tensor ** 2 를 사용할 수 있습니다.\n",
    "print('제곱 연산 결과: ', squared)\n",
    "\n",
    "# 절대값 연산\n",
    "abs_values = torch.abs(tensor)\n",
    "print('절대값 연산 결과: ', abs_values)\n",
    "\n",
    "# 역수 연산\n",
    "reciprocal = torch.reciprocal(tensor)\n",
    "print('역수 연산 결과: ', reciprocal)\n",
    "\n",
    "# 음수 연산\n",
    "neg = torch.neg(tensor)\n",
    "print('음수 연산 결과: ', neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08969088",
   "metadata": {},
   "source": [
    "## 5.3. 삼각함수\n",
    "- **sin**: 주어진 각도(라디안 단위)의 sin 값을 계산합니다. torch.sin(input) 함수를 사용합니다.\n",
    "\n",
    "- **cos**: 주어진 각도(라디안 단위)의 cos 값을 계산합니다. torch.cos(input) 함수를 사용합니다.\n",
    "\n",
    "- **tan**: 주어진 각도(라디안 단위)의 tan 값을 계산합니다. torch.tan(input) 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f53e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사인 값:  tensor([0.0000, 0.5000, 0.7071, 0.8660, 1.0000])\n",
      "코사인 값:  tensor([ 1.0000e+00,  8.6603e-01,  7.0711e-01,  5.0000e-01, -4.3711e-08])\n",
      "탄젠트 값:  tensor([ 0.0000e+00,  5.7735e-01,  1.0000e+00,  1.7321e+00, -2.2877e+07])\n"
     ]
    }
   ],
   "source": [
    "# 라디안 단위의 각도 Tensor 생성\n",
    "angles = torch.tensor([0, torch.pi/6, torch.pi/4, torch.pi/3, torch.pi/2])\n",
    "\n",
    "# 사인 연산\n",
    "sin_values = torch.sin(angles)\n",
    "print('사인 값: ', sin_values)\n",
    "\n",
    "# 코사인 연산\n",
    "cos_values = torch.cos(angles)\n",
    "print('코사인 값: ', cos_values)\n",
    "\n",
    "# 탄젠트 연산\n",
    "tan_values = torch.tan(angles)\n",
    "print('탄젠트 값: ', tan_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
