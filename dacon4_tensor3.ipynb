{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a2a0ad",
   "metadata": {},
   "source": [
    "# Stage 4. Tensor의 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272eee06",
   "metadata": {},
   "source": [
    "# 1. 기본 연산\n",
    "## 1.1. Tensor와 Scalar의 연산\n",
    "PyTorch에서 Tensor와 Scalar(단일 숫자)간의 사칙연산은 지원됩니다. Tensor와 Scalar 사이의 덧셈 연산을 수행할 때, Scalar 값은 Tensor의 모든 요소에 대해 적용됩니다. \\\n",
    "\\\n",
    "연산 방법은 Python과 동일합니다.\n",
    "- 덧셈: +\n",
    "- 뺄셈: -\n",
    "- 곱셈: *\n",
    "- 나눗셈: /\n",
    "- 제곱: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bbbf02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar 덧셈: \n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]])\n",
      "Scalar 뺄셈: \n",
      " tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "Scalar 곱셈: \n",
      " tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "Scalar 나눗셈: \n",
      " tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "Scalar 제곱: \n",
      " tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "\n",
    "# Tensor와 Scalar의 덧셈\n",
    "add_scalar = tensor + 1\n",
    "\n",
    "# Tensor와 Scalar의 뺄셈\n",
    "sub_scalar = tensor - 1\n",
    "\n",
    "# Tensor와 Scalar의 곱셈\n",
    "mul_scalar = tensor * 2\n",
    "\n",
    "# Tensor와 Scalar의 나눗셈\n",
    "div_scalar = tensor / 2\n",
    "\n",
    "# Tensor와 Scalar의 제곱\n",
    "pow_scalar = tensor ** 2\n",
    "\n",
    "print('Scalar 덧셈: \\n', add_scalar)\n",
    "print('Scalar 뺄셈: \\n', sub_scalar)\n",
    "print('Scalar 곱셈: \\n', mul_scalar)\n",
    "print('Scalar 나눗셈: \\n', div_scalar)\n",
    "print('Scalar 제곱: \\n', pow_scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11816d",
   "metadata": {},
   "source": [
    "## 1.2. 덧셈과 뺄셈\n",
    "Tensor의 덧셈과 뺄셈 연산은 두 Tensor의 동일 위치에 있는 요소들 끼리 연산을 수행합니다.\\\n",
    "\\\n",
    "예를 들어, 두 개의 2x2 텐서 tensor_a와 tensor_b는 각각 [[1,2],[3,4]]와 [[5,6],[7,8]]의 값을 갖을 때, 이 둘의 연산은 아래와 같이 계산됩니다.\n",
    "- 덧셈 결과: 각 요소를 더한 결과인 [[6,8],[10,12]]를 얻습니다. 이는 tensor_a의 각 요소에 tensor_b의 해당 요소를 더하여 계산된 결과입니다.\n",
    "- 뺄셈 결과: tensor_a에서 tensor_b를 뺀 결과인 [[-4,-4],[-4,-4]]를 얻습니다. 이는 tensor_a의 각 요소에서 tensor_b의 해당 요소를 빼서 계산된 결과입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d6ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "덧셈 결과: \n",
      " tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "뺄셈 결과: \n",
      " tensor([[-4, -4],\n",
      "        [-4, -4]])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([[1,2], [3,4]])\n",
    "tensor_b = torch.tensor([[5,6], [7,8]])\n",
    "\n",
    "# Tensor 덧셈\n",
    "add_result = tensor_a + tensor_b\n",
    "# 또는 torch.add(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# Tensor 뺄셈\n",
    "sub_result = tensor_a - tensor_b\n",
    "# 또는 torch.sub(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('덧셈 결과: \\n', add_result)\n",
    "print('뺄셈 결과: \\n', sub_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a6cd1",
   "metadata": {},
   "source": [
    "## 1.3. 곱셈과 나눗셈\n",
    "요소별 곱셈과 나눗셈 연산도 마찬가지로 각 텐서의 동일한 위치에 있는 요소끼리 수행됩니다.\\\n",
    "\\\n",
    "예를 들어, tensor_a와 tensor_b는 각각 [2,3,4]와 [5,6,7]의 값을 갖는 Tensor입니다. 연산 결과는 다음과 같습니다.\n",
    "- 요소별 곱셈 결과: 각 요소의 곱셈 결과인 [10, 18, 28]을 얻습니다.\n",
    "- 요소별 나눗셈 결과: 각 요소를 나눈 결과인 [0.4, 0.5, 0.5714...]를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26b532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요소별 곱셈:  tensor([10., 18., 28.])\n",
      "요소별 나눗셈:  tensor([0.4000, 0.5000, 0.5714])\n"
     ]
    }
   ],
   "source": [
    "# 두 Tensor 생성\n",
    "tensor_a = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
    "tensor_b = torch.tensor([5, 6, 7], dtype=torch.float32)\n",
    "\n",
    "# 요소별 곱셈\n",
    "product = tensor_a * tensor_b\n",
    "# 또는 torch.mul(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "print('요소별 곱셈: ', product)\n",
    "\n",
    "# 요소별 나눗셈\n",
    "division = tensor_a / tensor_b\n",
    "# 또는 torch.div(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "print('요소별 나눗셈: ', division)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141c460",
   "metadata": {},
   "source": [
    "## 1.4. Broadcasting 이해하기\n",
    "Broadcasting을 통해 서로 다른 shape을 가진 Tensor들 간에도 수학 연산을 수행할 수 있게 됩니다. 이는 더 작은 Tensor가 큰 Tensor의 모양에 맞게 자동으로 확장(확대)되어 연산이 가능하게 만듭니다. \\\n",
    "\\\n",
    "Broadcasting이 작동하는 방법은 다음과 같은 규칙을 따릅니다.\n",
    "1. 차원의 크기가 같거나, 하나의 차원이 1인 경우에만 Broadcasting이 가능합니다. 예를 들어, (5, 4) 모양의 Tensor와 (1, 4) 모양의 Tensor는 Broadcasting이 가능합니다. 여기서 (1, 4) 모양의 Tensor는 첫 번째 차원을 따라 5회 반복되어 (5, 4) 모양으로 확장됩니다.\n",
    "2. Tensor의 차원 수가 다를 경우, 더 작은 차원을 가진 Tensor의 모양 앞에 1을 추가하여 차원의 수를 맞춥니다. 예를 들어, (5, 4) 모양의 Tensor와 (4,) 모양의 Tensor가 있을 때, 더 작은 Tensor는 (1, 4)로 간주됩니다. 그 후, 앞서 설명한 바와 같이 Broadcasting이 수행됩니다.\n",
    "3. Broadcasting은 각 차원을 따라 반복함으로써 더 큰 모양의 Tensor에 맞추어 확장합니다. 이 과정은 실제 data 복사가 일어나지 않으며, 연산을 효율적으로 만들기 위한 가상의 확장으로 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1c5d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요소별 덧셈 결과: \n",
      " tensor([[5., 7., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 (1, 3)인 Tensor 생성\n",
    "tensor_a = torch.tensor([[1, 2, 3]], dtype=torch.float32)  \n",
    "\n",
    "# 크기가 (3,)인 Tensor 생성\n",
    "tensor_b = torch.tensor([4, 5, 6], dtype=torch.float32)   \n",
    "\n",
    "# tensor_a와 tensor_b의 요소별 덧셈 (Broadcasting 발생)\n",
    "# tensor_b가 tensor_a의 모양에 맞게 확장되어 연산됨\n",
    "result_add = tensor_a + tensor_b\n",
    "print('요소별 덧셈 결과: \\n', result_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1051302",
   "metadata": {},
   "source": [
    "# 2. 비교 연산\n",
    "## 2.1. 동등 비교\n",
    "동등 비교는 두 Tensor 간의 요소별 동등성을 비교하는 데 사용하는 것입니다. \\\n",
    "\\\n",
    "동등비교를 위해 ==또는 torch.eq()를 사용합니다. 이 함수는 두 Tensor의 동일한 위치에 있는 요소가 같은지 여부를 검사하고, 결과를 Boolean Tensor로 반환합니다. 즉, 두 요소가 같으면 True, 다르면 False 값을 갖습니다. \\\n",
    "\\\n",
    "torch.eq() 함수의 사용법은 매우 간단합니다. 두 Tensor a와 b가 주어졌을 때, torch.eq(a, b)를 호출하여 두 Tensor 간의 요소별 동등 비교를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c05cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동등 비교 결과:  tensor([ True,  True, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([1, 2, 3, 4, 5])\n",
    "tensor_b = torch.tensor([1, 2, 0, 4, 5])\n",
    "\n",
    "result = tensor_a == tensor_b\n",
    "# torch.eq(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('동등 비교 결과: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732cb79",
   "metadata": {},
   "source": [
    "## 2.2. 대소 비교(1) - 크다/작다\n",
    "동등 비교는 두 Tensor 간의 요소별 동등성을 비교하는 데 사용하는 것입니다. \\\n",
    "\\\n",
    "동등 비교를 위해 == 또는 torch.eq()를 사용합니다. 이 함수는 두 Tensor의 동일한 위치에 있는 요소가 같은지 여부를 검사하고, 결과를 Boolean Tensor로 반환합니다. 즉, 두 요소가 같으면 True, 다르면 False 값을 갖습니다. \\\n",
    "\\\n",
    "torch.eq() 함수의 사용법은 매우 간단합니다. 두 Tensor a와 b가 주어졌을 때, torch.eq(a, b)를 호출하여 두 Tensor 간의 요소별 동등 비교를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa45881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a가 b보다 큰가:  tensor([ True, False, False, False])\n",
      "a가 b보다 작은가:  tensor([False, False, False,  True])\n"
     ]
    }
   ],
   "source": [
    "tensor_a = torch.tensor([5, 6, 7, 8])\n",
    "tensor_b = torch.tensor([4, 6, 7, 10])\n",
    "\n",
    "# 조건: a 텐서가 b 텐서보다 크다\n",
    "gt_result = tensor_a > tensor_b\n",
    "# 또는 torch.gt(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "# 조건: a 텐서가 b 텐서보다 작다\n",
    "lt_result = tensor_a < tensor_b\n",
    "# 또는 torch.lt(tensor_a, tensor_b)를 사용할 수 있습니다.\n",
    "\n",
    "print('a가 b보다 큰가: ', gt_result)\n",
    "print('a가 b보다 작은가: ', lt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d831a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
